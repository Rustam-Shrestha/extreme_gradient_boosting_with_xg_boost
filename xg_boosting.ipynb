{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c1260ac-4d7a-4af8-ba0d-384e2bad6dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7743f3-554e-4a55-bafd-3d9677a7d8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30019/4072909886.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  churn_data['TotalCharges'].fillna(churn_data['TotalCharges'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load the dataset (make sure the CSV is in your working directory)\n",
    "churn_data = pd.read_csv('Telco-Customer-Churn.csv')\n",
    "\n",
    "# Step 2: Drop unnecessary column (customerID is not useful for prediction)\n",
    "churn_data = churn_data.drop('customerID', axis=1)\n",
    "\n",
    "# Step 3: Convert TotalCharges to numeric (it's stored as string; some values are blank)\n",
    "churn_data['TotalCharges'] = pd.to_numeric(churn_data['TotalCharges'], errors='coerce')\n",
    "# Fill NaN in TotalCharges (only 11 rows) with median\n",
    "churn_data['TotalCharges'].fillna(churn_data['TotalCharges'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d2f385-9f2d-4890-8dd6-f98d08df8723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rustamshrestha/jupyterenv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.784244\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in churn_data.columns:\n",
    "    if churn_data[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        churn_data[column] = le.fit_transform(churn_data[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "# Now all columns are numeric\n",
    "X = churn_data.iloc[:, :-1]  # All features except last column\n",
    "y = churn_data.iloc[:, -1]   # Last column is 'Churn' (0 = No, 1 = Yes)\n",
    "\n",
    "# Step 5: Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=123\n",
    ")\n",
    "\n",
    "# Step 6: Train XGBoost classifier\n",
    "xg_cl = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    n_estimators=100,  # Increased for better performance\n",
    "    seed=123,\n",
    "    use_label_encoder=False,  # Suppress warning\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predict and evaluate\n",
    "preds = xg_cl.predict(X_test)\n",
    "accuracy = float(np.sum(preds == y_test)) / y_test.shape[0]\n",
    "print(\"Accuracy: %f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7487f10-46f3-44ab-9e02-3a0cee55db33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the DecisionTreeClassifier with max_depth=4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4, random_state=123)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = float(np.sum(y_pred_4 == y_test)) / y_test.shape[0]\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6c0971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-error-mean  train-error-std  test-error-mean  test-error-std\n",
      "0          0.055358         0.014118         0.098459        0.016569\n",
      "1          0.026367         0.003758         0.061524        0.013876\n",
      "2          0.012302         0.001236         0.061533        0.013930\n",
      "3          0.011427         0.002497         0.052752        0.015618\n",
      "4          0.008790         0.002494         0.052752        0.015618\n",
      "Accuracy: 0.9472\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create the DMatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\": \"binary:logistic\", \"max_depth\": 3}\n",
    "\n",
    "# Perform 3-fold cross-validation\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, \n",
    "                    metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {((1 - cv_results['test-error-mean']).iloc[-1]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba514a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
      "0        0.989659       0.005199       0.959695      0.025255\n",
      "1        0.995100       0.003751       0.972271      0.023940\n",
      "2        0.997122       0.002032       0.973122      0.025047\n",
      "3        0.997103       0.002030       0.982087      0.013069\n",
      "4        0.997832       0.001851       0.982567      0.013554\n",
      "AUC: 0.9826\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create the DMatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\": \"binary:logistic\", \"max_depth\": 3}\n",
    "\n",
    "# Perform 3-fold cross-validation with AUC metric\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, \n",
    "                    metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print(f\"AUC: {(cv_results['test-auc-mean']).iloc[-1]:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fd3d9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 45.1356\n",
      "MAE: 35.6638\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Generate synthetic regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=4, noise=0.1, random_state=123)\n",
    "\n",
    "# Split the data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define parameters\n",
    "params = {\"objective\": \"reg:squarederror\", \"max_depth\": 3, \"eta\": 0.1}\n",
    "\n",
    "# Train the model\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=10)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = xgb_model.predict(dtest)\n",
    "\n",
    "# Compute RMSE and MAE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4910dd1-a561-4d43-ba63-1507cb44416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import xgboost as xgb                 # XGBoost library for gradient boosting\n",
    "import pandas as pd                   # For data manipulation\n",
    "import numpy as np                    # For numerical operations\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  # For evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "021100f3-75db-4fce-9aea-63370474cb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Metrics:\n",
      "Mean Absolute Error (MAE): 2.85\n",
      "Mean Squared Error (MSE): 21.20\n",
      "R² Score: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = boston_data.iloc[:, :-1]  # All columns except the last\n",
    "y = boston_data.iloc[:, -1]   # Last column as target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=10, seed=123)\n",
    "\n",
    "# Train the model\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mae = mean_absolute_error(y_test, preds)\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)\n",
    "\n",
    "# Print accuracy metrics\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3266c47-fb00-408d-97a6-4cb2b5a6a2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Metrics:\n",
      "Mean Absolute Error (MAE): 4.19\n",
      "Mean Squared Error (MSE): 37.06\n",
      "R² Score: 0.55\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = boston_data.iloc[:, :-1]  # All columns except the last\n",
    "y = boston_data.iloc[:, -1]   # Last column as target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "\n",
    "DM_test = xgb.DMatrix(data=X_test, label=y_test)  # Use y_test here instead of y_train\n",
    "\n",
    "params= {\"booster\":\"gblinear\", \"objective\":\"reg:squarederror\"}\n",
    "xg_reg = xgb.train(params=params, dtrain=DM_train, num_boost_round=10)\n",
    "\n",
    "preds =xg_reg.predict(DM_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mae = mean_absolute_error(y_test, preds)\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)\n",
    "\n",
    "# Print accuracy metrics\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83420eaa-e1bc-41c6-a38d-0e05404e3067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.604776\n"
     ]
    }
   ],
   "source": [
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "287bd72c-b673-44b0-b8f9-910303eff2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 6.454039\n"
     ]
    }
   ],
   "source": [
    "# Convert the training and testing sets into DMatrixes\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "DM_test = xgb.DMatrix(data=X_test, label=y_test) \n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"booster\": \"gblinear\", \"objective\": \"reg:squarederror\"} \n",
    "\n",
    "# Train the model\n",
    "xg_reg = xgb.train(params=params, dtrain=DM_train, num_boost_round=5)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xg_reg.predict(DM_test) #\n",
    "\n",
    "# Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "973620e2-9e6c-42bc-ae9f-af04fa746f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0         6.989180        0.032758        7.188751       0.188563\n",
      "1         5.459640        0.032484        5.959252       0.216005\n",
      "2         4.369413        0.028176        5.150507       0.258300\n",
      "3         3.612622        0.043512        4.570613       0.344081\n",
      "4         3.071021        0.044801        4.296591       0.447778\n",
      "4    4.296591\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30417be6-21a9-40d6-9106-716a2a6f52c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE as a function of alpha:\n",
      "   alpha      rmse\n",
      "0      1  3.685441\n",
      "1     10  3.761246\n",
      "2    100  4.461392\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = boston_data.iloc[:, :-1]\n",
    "y = boston_data.iloc[:, -1]\n",
    "\n",
    "# Create DMatrix\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Set base parameters\n",
    "params = {\"objective\": \"reg:squarederror\", \"max_depth\": 4}\n",
    "\n",
    "# L1 regularization values to test\n",
    "l1_params = [1, 10, 100]\n",
    "rmses_l1 = []\n",
    "\n",
    "# Loop through each alpha value\n",
    "for reg in l1_params:\n",
    "    params[\"alpha\"] = reg  # Set L1 regularization\n",
    "    cv_results = xgb.cv(\n",
    "        dtrain=boston_dmatrix,\n",
    "        params=params,\n",
    "        nfold=4,\n",
    "        num_boost_round=10,\n",
    "        metrics=\"rmse\",\n",
    "        as_pandas=True,\n",
    "        seed=123\n",
    "    )\n",
    "    # Extract final RMSE\n",
    "    final_rmse = cv_results[\"test-rmse-mean\"].tail(1).values[0]\n",
    "    rmses_l1.append(final_rmse)\n",
    "\n",
    "# Display results\n",
    "print(\"Best RMSE as a function of alpha:\")\n",
    "print(pd.DataFrame(list(zip(l1_params, rmses_l1)), columns=[\"alpha\", \"rmse\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242327e4-7d41-4c46-a3fb-20aea3b00e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":2}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot all trees in a loop\n",
    "for i in range(10):\n",
    "    xgb.plot_tree(xg_reg, num_trees=i)\n",
    "    plt.title(f\"Tree {i}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cadd10-984a-40cd-bc01-cdee82e69ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the feature importances\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668cb07c-54d0-4ad8-bb24-e5c02cf7858a",
   "metadata": {},
   "source": [
    "# Day 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af8fb66-c9f7-41e0-b1bb-1524521ee894",
   "metadata": {},
   "source": [
    "## tuning parameters manually (tuning boosting rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454217f9-bbaf-45a2-ac78-f3ed232b7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the number of boosting rounds\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\": \"reg:squarederror\", \"max_depth\": 3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses, columns=[\"num_boosting_rounds\", \"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e2e2b-c9a2-45e3-b707-1d0c5cc2b965",
   "metadata": {},
   "source": [
    "## # Automated boosting round selection using early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146edfec-5c7a-462d-a814-3b7587058e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated boosting round selection using early stopping\n",
    "\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\": \"reg:squarederror\", \"max_depth\": 4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(\n",
    "    dtrain=housing_dmatrix,\n",
    "    params=params,\n",
    "    nfold=3,\n",
    "    num_boost_round=50,\n",
    "    early_stopping_rounds=10,\n",
    "    metrics=\"rmse\",\n",
    "    as_pandas=True,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de20569-9dda-440b-a42e-db5fc886928d",
   "metadata": {},
   "source": [
    "## ETA (shrinkage ) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "14b8c1f6-b3b6-4cd8-93bf-9d143582ceab",
   "metadata": {},
   "source": [
    "# Tuning eta\n",
    "\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\": \"reg:squarederror\", \"max_depth\": 3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in eta_vals:\n",
    "    params[\"eta\"] = curr_val\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(\n",
    "        dtrain=housing_dmatrix,\n",
    "        params=params,\n",
    "        nfold=3,\n",
    "        num_boost_round=10,\n",
    "        early_stopping_rounds=5,\n",
    "        metrics=\"rmse\",\n",
    "        as_pandas=True,\n",
    "        seed=123\n",
    "    )\n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\", \"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a463dcd-0dec-41f5-9af9-feb372a22cd6",
   "metadata": {},
   "source": [
    "## tuning max depth of tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5df86e-c33c-4d54-b3a3-ae27c87ea1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning max_depth\n",
    "\n",
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\": \"reg:squarederror\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2, 5, 10, 20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(\n",
    "        dtrain=housing_dmatrix,\n",
    "        params=params,\n",
    "        nfold=2,\n",
    "        num_boost_round=10,\n",
    "        early_stopping_rounds=5,\n",
    "        metrics=\"rmse\",\n",
    "        seed=123,\n",
    "        as_pandas=True\n",
    "    )\n",
    "    # Append the final round RMSE to best_rmse \n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)), columns=[\"max_depth\", \"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a742c8-5c67-4828-b6cf-49c6a60105d4",
   "metadata": {},
   "source": [
    "## tuning colsample by tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817f472-1595-457d-ba41-662f8b801eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning colsample_bytree\n",
    "\n",
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\": \"reg:squarederror\", \"max_depth\": 3}\n",
    "\n",
    "# Create list of hyperparameter values: colsample_bytree_vals\n",
    "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "    params[\"colsample_bytree\"] = curr_val\n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(\n",
    "        dtrain=housing_dmatrix, \n",
    "        params=params, \n",
    "        nfold=2,\n",
    "        num_boost_round=10, \n",
    "        early_stopping_rounds=5,\n",
    "        metrics=\"rmse\", \n",
    "        as_pandas=True, \n",
    "        seed=123\n",
    "    )\n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\", \"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555413e1-a675-45ce-8f0b-8c5423f7e004",
   "metadata": {},
   "source": [
    "## girdsearch cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41911cfa-3f67-4ca3-82e9-0321edcda771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search with XGBoost\n",
    "\n",
    "import pandas as pd               # Load pandas for data manipulation\n",
    "import xgboost as xgb             # Load XGBoost library\n",
    "import numpy as np                # Load NumPy for numerical operations\n",
    "from sklearn.model_selection import GridSearchCV  # Import GridSearchCV for hyperparameter tuning\n",
    "\n",
    "housing_data = pd.read_csv(\"ames_housing_trimmed_processed.csv\")  \n",
    "# Load the preprocessed Ames housing dataset from CSV into a DataFrame\n",
    "\n",
    "X, y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]  \n",
    "# Split the dataset into features (X) and target (y). Assumes last column is the target variable.\n",
    "\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)  \n",
    "# Convert the data into XGBoost’s optimized DMatrix format (optional for scikit-learn interface, but useful for native XGBoost)\n",
    "\n",
    "gbm_param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 0.9],   # Try 4 different learning rates (eta)\n",
    "    'n_estimators': [200],                   # Fix number of boosting rounds to 200\n",
    "    'subsample': [0.3, 0.5, 0.9]             # Try 3 different subsample ratios\n",
    "}\n",
    "# Define the grid of hyperparameters to search over. Total combinations = 4 × 1 × 3 = 12\n",
    "\n",
    "gbm = xgb.XGBRegressor()  \n",
    "# Create an XGBoost regressor object using scikit-learn API\n",
    "\n",
    "grid_mse = GridSearchCV(\n",
    "    estimator=gbm,                   # The model to tune\n",
    "    param_grid=gbm_param_grid,       # The hyperparameter grid\n",
    "    scoring='neg_mean_squared_error',# Use negative MSE as scoring (scikit-learn convention)\n",
    "    cv=4,                            # Use 4-fold cross-validation\n",
    "    verbose=1                        # Print progress during training\n",
    ")\n",
    "# Set up the grid search with cross-validation and scoring metric\n",
    "grid_mse.fit(X, y)  \n",
    "# Fit the grid search object to the data. Trains 12 models (one for each parameter combo) using 4-fold CV.\n",
    "\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)  \n",
    "# Print the best combination of hyperparameters found during grid search\n",
    "\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))  \n",
    "# Convert the best negative MSE score to RMSE and print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502fc10-8ed3-4169-8884-1a78837bc9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b426ee-2696-4490-85c2-bf54c07a254e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83a7b4-4b61-4a89-bfde-3078af72172a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d168bd-7c55-49a8-b128-171d8d856ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517e1d7-4ebb-42ad-99ab-b55f67512ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90f374-2f19-40c3-910b-c906561f88e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb083a-dabf-4c1f-bc3c-38cd96fcf5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32098243-ebab-4637-9793-a6280a65ad15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4098c0d3-f2f3-4738-8629-3d38e80aef47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6c91a-d18f-48b6-a201-aec8fdad92bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa3c823-07b6-463d-86b8-08dbb5cc4e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1c134-7e7f-46a4-8116-3403d6e2a25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc897cd5-c5ff-4390-8a6b-c18b7b7f4fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85bdd21-c10d-4481-8741-477424b9946f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b887ef9-2b41-48ee-b55d-1185c77a538f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fa938a-ccda-4402-b329-ec0db066fe86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28768e33-7efa-4ec0-9264-36291c7c8fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
