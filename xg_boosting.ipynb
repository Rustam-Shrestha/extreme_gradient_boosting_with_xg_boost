{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c1260ac-4d7a-4af8-ba0d-384e2bad6dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7743f3-554e-4a55-bafd-3d9677a7d8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30019/4072909886.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  churn_data['TotalCharges'].fillna(churn_data['TotalCharges'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load the dataset (make sure the CSV is in your working directory)\n",
    "churn_data = pd.read_csv('Telco-Customer-Churn.csv')\n",
    "\n",
    "# Step 2: Drop unnecessary column (customerID is not useful for prediction)\n",
    "churn_data = churn_data.drop('customerID', axis=1)\n",
    "\n",
    "# Step 3: Convert TotalCharges to numeric (it's stored as string; some values are blank)\n",
    "churn_data['TotalCharges'] = pd.to_numeric(churn_data['TotalCharges'], errors='coerce')\n",
    "# Fill NaN in TotalCharges (only 11 rows) with median\n",
    "churn_data['TotalCharges'].fillna(churn_data['TotalCharges'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d2f385-9f2d-4890-8dd6-f98d08df8723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rustamshrestha/jupyterenv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.784244\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in churn_data.columns:\n",
    "    if churn_data[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        churn_data[column] = le.fit_transform(churn_data[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "# Now all columns are numeric\n",
    "X = churn_data.iloc[:, :-1]  # All features except last column\n",
    "y = churn_data.iloc[:, -1]   # Last column is 'Churn' (0 = No, 1 = Yes)\n",
    "\n",
    "# Step 5: Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=123\n",
    ")\n",
    "\n",
    "# Step 6: Train XGBoost classifier\n",
    "xg_cl = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    n_estimators=100,  # Increased for better performance\n",
    "    seed=123,\n",
    "    use_label_encoder=False,  # Suppress warning\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predict and evaluate\n",
    "preds = xg_cl.predict(X_test)\n",
    "accuracy = float(np.sum(preds == y_test)) / y_test.shape[0]\n",
    "print(\"Accuracy: %f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7487f10-46f3-44ab-9e02-3a0cee55db33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the DecisionTreeClassifier with max_depth=4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4, random_state=123)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = float(np.sum(y_pred_4 == y_test)) / y_test.shape[0]\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6c0971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-error-mean  train-error-std  test-error-mean  test-error-std\n",
      "0          0.055358         0.014118         0.098459        0.016569\n",
      "1          0.026367         0.003758         0.061524        0.013876\n",
      "2          0.012302         0.001236         0.061533        0.013930\n",
      "3          0.011427         0.002497         0.052752        0.015618\n",
      "4          0.008790         0.002494         0.052752        0.015618\n",
      "Accuracy: 0.9472\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create the DMatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\": \"binary:logistic\", \"max_depth\": 3}\n",
    "\n",
    "# Perform 3-fold cross-validation\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, \n",
    "                    metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {((1 - cv_results['test-error-mean']).iloc[-1]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba514a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
      "0        0.989659       0.005199       0.959695      0.025255\n",
      "1        0.995100       0.003751       0.972271      0.023940\n",
      "2        0.997122       0.002032       0.973122      0.025047\n",
      "3        0.997103       0.002030       0.982087      0.013069\n",
      "4        0.997832       0.001851       0.982567      0.013554\n",
      "AUC: 0.9826\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create the DMatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\": \"binary:logistic\", \"max_depth\": 3}\n",
    "\n",
    "# Perform 3-fold cross-validation with AUC metric\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, \n",
    "                    metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print(f\"AUC: {(cv_results['test-auc-mean']).iloc[-1]:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fd3d9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 45.1356\n",
      "MAE: 35.6638\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Generate synthetic regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=4, noise=0.1, random_state=123)\n",
    "\n",
    "# Split the data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define parameters\n",
    "params = {\"objective\": \"reg:squarederror\", \"max_depth\": 3, \"eta\": 0.1}\n",
    "\n",
    "# Train the model\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=10)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = xgb_model.predict(dtest)\n",
    "\n",
    "# Compute RMSE and MAE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4910dd1-a561-4d43-ba63-1507cb44416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import xgboost as xgb                 # XGBoost library for gradient boosting\n",
    "import pandas as pd                   # For data manipulation\n",
    "import numpy as np                    # For numerical operations\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  # For evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "021100f3-75db-4fce-9aea-63370474cb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Metrics:\n",
      "Mean Absolute Error (MAE): 2.85\n",
      "Mean Squared Error (MSE): 21.20\n",
      "R² Score: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = boston_data.iloc[:, :-1]  # All columns except the last\n",
    "y = boston_data.iloc[:, -1]   # Last column as target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=10, seed=123)\n",
    "\n",
    "# Train the model\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mae = mean_absolute_error(y_test, preds)\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)\n",
    "\n",
    "# Print accuracy metrics\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3266c47-fb00-408d-97a6-4cb2b5a6a2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Metrics:\n",
      "Mean Absolute Error (MAE): 4.19\n",
      "Mean Squared Error (MSE): 37.06\n",
      "R² Score: 0.55\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = boston_data.iloc[:, :-1]  # All columns except the last\n",
    "y = boston_data.iloc[:, -1]   # Last column as target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "\n",
    "DM_test = xgb.DMatrix(data=X_test, label=y_test)  # Use y_test here instead of y_train\n",
    "\n",
    "params= {\"booster\":\"gblinear\", \"objective\":\"reg:squarederror\"}\n",
    "xg_reg = xgb.train(params=params, dtrain=DM_train, num_boost_round=10)\n",
    "\n",
    "preds =xg_reg.predict(DM_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mae = mean_absolute_error(y_test, preds)\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)\n",
    "\n",
    "# Print accuracy metrics\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83420eaa-e1bc-41c6-a38d-0e05404e3067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.604776\n"
     ]
    }
   ],
   "source": [
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "287bd72c-b673-44b0-b8f9-910303eff2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 6.454039\n"
     ]
    }
   ],
   "source": [
    "# Convert the training and testing sets into DMatrixes\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "DM_test = xgb.DMatrix(data=X_test, label=y_test) \n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"booster\": \"gblinear\", \"objective\": \"reg:squarederror\"} \n",
    "\n",
    "# Train the model\n",
    "xg_reg = xgb.train(params=params, dtrain=DM_train, num_boost_round=5)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xg_reg.predict(DM_test) #\n",
    "\n",
    "# Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "973620e2-9e6c-42bc-ae9f-af04fa746f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0         6.989180        0.032758        7.188751       0.188563\n",
      "1         5.459640        0.032484        5.959252       0.216005\n",
      "2         4.369413        0.028176        5.150507       0.258300\n",
      "3         3.612622        0.043512        4.570613       0.344081\n",
      "4         3.071021        0.044801        4.296591       0.447778\n",
      "4    4.296591\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30417be6-21a9-40d6-9106-716a2a6f52c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE as a function of alpha:\n",
      "   alpha      rmse\n",
      "0      1  3.685441\n",
      "1     10  3.761246\n",
      "2    100  4.461392\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = boston_data.iloc[:, :-1]\n",
    "y = boston_data.iloc[:, -1]\n",
    "\n",
    "# Create DMatrix\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Set base parameters\n",
    "params = {\"objective\": \"reg:squarederror\", \"max_depth\": 4}\n",
    "\n",
    "# L1 regularization values to test\n",
    "l1_params = [1, 10, 100]\n",
    "rmses_l1 = []\n",
    "\n",
    "# Loop through each alpha value\n",
    "for reg in l1_params:\n",
    "    params[\"alpha\"] = reg  # Set L1 regularization\n",
    "    cv_results = xgb.cv(\n",
    "        dtrain=boston_dmatrix,\n",
    "        params=params,\n",
    "        nfold=4,\n",
    "        num_boost_round=10,\n",
    "        metrics=\"rmse\",\n",
    "        as_pandas=True,\n",
    "        seed=123\n",
    "    )\n",
    "    # Extract final RMSE\n",
    "    final_rmse = cv_results[\"test-rmse-mean\"].tail(1).values[0]\n",
    "    rmses_l1.append(final_rmse)\n",
    "\n",
    "# Display results\n",
    "print(\"Best RMSE as a function of alpha:\")\n",
    "print(pd.DataFrame(list(zip(l1_params, rmses_l1)), columns=[\"alpha\", \"rmse\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242327e4-7d41-4c46-a3fb-20aea3b00e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":2}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot all trees in a loop\n",
    "for i in range(10):\n",
    "    xgb.plot_tree(xg_reg, num_trees=i)\n",
    "    plt.title(f\"Tree {i}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cadd10-984a-40cd-bc01-cdee82e69ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the feature importances\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668cb07c-54d0-4ad8-bb24-e5c02cf7858a",
   "metadata": {},
   "source": [
    "# Day 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af8fb66-c9f7-41e0-b1bb-1524521ee894",
   "metadata": {},
   "source": [
    "## tuning parameters manually (tuning boosting rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454217f9-bbaf-45a2-ac78-f3ed232b7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the number of boosting rounds\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\": \"reg:squarederror\", \"max_depth\": 3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses, columns=[\"num_boosting_rounds\", \"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e2e2b-c9a2-45e3-b707-1d0c5cc2b965",
   "metadata": {},
   "source": [
    "## # Automated boosting round selection using early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146edfec-5c7a-462d-a814-3b7587058e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated boosting round selection using early stopping\n",
    "\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\": \"reg:squarederror\", \"max_depth\": 4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(\n",
    "    dtrain=housing_dmatrix,\n",
    "    params=params,\n",
    "    nfold=3,\n",
    "    num_boost_round=50,\n",
    "    early_stopping_rounds=10,\n",
    "    metrics=\"rmse\",\n",
    "    as_pandas=True,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de20569-9dda-440b-a42e-db5fc886928d",
   "metadata": {},
   "source": [
    "## ETA (shrinkage ) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "14b8c1f6-b3b6-4cd8-93bf-9d143582ceab",
   "metadata": {},
   "source": [
    "# Tuning eta\n",
    "\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\": \"reg:squarederror\", \"max_depth\": 3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in eta_vals:\n",
    "    params[\"eta\"] = curr_val\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(\n",
    "        dtrain=housing_dmatrix,\n",
    "        params=params,\n",
    "        nfold=3,\n",
    "        num_boost_round=10,\n",
    "        early_stopping_rounds=5,\n",
    "        metrics=\"rmse\",\n",
    "        as_pandas=True,\n",
    "        seed=123\n",
    "    )\n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\", \"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a463dcd-0dec-41f5-9af9-feb372a22cd6",
   "metadata": {},
   "source": [
    "## tuning max depth of tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5df86e-c33c-4d54-b3a3-ae27c87ea1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning max_depth\n",
    "\n",
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\": \"reg:squarederror\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2, 5, 10, 20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(\n",
    "        dtrain=housing_dmatrix,\n",
    "        params=params,\n",
    "        nfold=2,\n",
    "        num_boost_round=10,\n",
    "        early_stopping_rounds=5,\n",
    "        metrics=\"rmse\",\n",
    "        seed=123,\n",
    "        as_pandas=True\n",
    "    )\n",
    "    # Append the final round RMSE to best_rmse \n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)), columns=[\"max_depth\", \"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a742c8-5c67-4828-b6cf-49c6a60105d4",
   "metadata": {},
   "source": [
    "## tuning colsample by tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817f472-1595-457d-ba41-662f8b801eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning colsample_bytree\n",
    "\n",
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\": \"reg:squarederror\", \"max_depth\": 3}\n",
    "\n",
    "# Create list of hyperparameter values: colsample_bytree_vals\n",
    "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "    params[\"colsample_bytree\"] = curr_val\n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(\n",
    "        dtrain=housing_dmatrix, \n",
    "        params=params, \n",
    "        nfold=2,\n",
    "        num_boost_round=10, \n",
    "        early_stopping_rounds=5,\n",
    "        metrics=\"rmse\", \n",
    "        as_pandas=True, \n",
    "        seed=123\n",
    "    )\n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\", \"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555413e1-a675-45ce-8f0b-8c5423f7e004",
   "metadata": {},
   "source": [
    "## girdsearch cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41911cfa-3f67-4ca3-82e9-0321edcda771",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:MS Zoning: object, Street: object, Alley: object, Lot Shape: object, Land Contour: object, Utilities: object, Lot Config: object, Land Slope: object, Neighborhood: object, Condition 1: object, Condition 2: object, Bldg Type: object, House Style: object, Roof Style: object, Roof Matl: object, Exterior 1st: object, Exterior 2nd: object, Mas Vnr Type: object, Exter Qual: object, Exter Cond: object, Foundation: object, Bsmt Qual: object, Bsmt Cond: object, Bsmt Exposure: object, BsmtFin Type 1: object, BsmtFin Type 2: object, Heating: object, Heating QC: object, Central Air: object, Electrical: object, Kitchen Qual: object, Functional: object, Fireplace Qu: object, Garage Type: object, Garage Finish: object, Garage Qual: object, Garage Cond: object, Paved Drive: object, Pool QC: object, Fence: object, Misc Feature: object, Sale Type: object, Sale Condition: object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyterenv/lib/python3.12/site-packages/xgboost/data.py:407\u001b[39m, in \u001b[36mpandas_feature_info\u001b[39m\u001b[34m(data, meta, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     new_feature_types.append(\u001b[43m_pandas_dtype_mapper\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'object'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m X, y = housing_data[housing_data.columns.tolist()[:-\u001b[32m1\u001b[39m]], housing_data[housing_data.columns.tolist()[-\u001b[32m1\u001b[39m]]  \n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Split the dataset into features (X) and target (y). Assumes last column is the target variable.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m housing_dmatrix = \u001b[43mxgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Convert the data into XGBoost’s optimized DMatrix format (optional for scikit-learn interface, but useful for native XGBoost)\u001b[39;00m\n\u001b[32m     17\u001b[39m gbm_param_grid = {\n\u001b[32m     18\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.01\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.9\u001b[39m],   \u001b[38;5;66;03m# Try 4 different learning rates (eta)\u001b[39;00m\n\u001b[32m     19\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m200\u001b[39m],                   \u001b[38;5;66;03m# Fix number of boosting rounds to 200\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msubsample\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.3\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.9\u001b[39m]             \u001b[38;5;66;03m# Try 3 different subsample ratios\u001b[39;00m\n\u001b[32m     21\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyterenv/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyterenv/lib/python3.12/site-packages/xgboost/core.py:885\u001b[39m, in \u001b[36mDMatrix.__init__\u001b[39m\u001b[34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[39m\n\u001b[32m    882\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    883\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m885\u001b[39m handle, feature_names, feature_types = \u001b[43mdispatch_data_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnthread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_split_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_split_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    895\u001b[39m \u001b[38;5;28mself\u001b[39m.handle = handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyterenv/lib/python3.12/site-packages/xgboost/data.py:1441\u001b[39m, in \u001b[36mdispatch_data_backend\u001b[39m\u001b[34m(data, missing, threads, feature_names, feature_types, enable_categorical, data_split_mode)\u001b[39m\n\u001b[32m   1439\u001b[39m     data = pd.DataFrame(data)\n\u001b[32m   1440\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[32m-> \u001b[39m\u001b[32m1441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_from_pandas_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1443\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnthread\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_split_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_split_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_cudf_df(data) \u001b[38;5;129;01mor\u001b[39;00m _is_cudf_ser(data):\n\u001b[32m   1451\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_cudf_df(\n\u001b[32m   1452\u001b[39m         data=data,\n\u001b[32m   1453\u001b[39m         missing=missing,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1457\u001b[39m         enable_categorical=enable_categorical,\n\u001b[32m   1458\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyterenv/lib/python3.12/site-packages/xgboost/data.py:674\u001b[39m, in \u001b[36m_from_pandas_df\u001b[39m\u001b[34m(data, enable_categorical, missing, nthread, feature_names, feature_types, data_split_mode)\u001b[39m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_from_pandas_df\u001b[39m(\n\u001b[32m    665\u001b[39m     *,\n\u001b[32m    666\u001b[39m     data: DataFrame,\n\u001b[32m   (...)\u001b[39m\u001b[32m    672\u001b[39m     data_split_mode: DataSplitMode = DataSplitMode.ROW,\n\u001b[32m    673\u001b[39m ) -> DispatchedDataBackendReturnType:\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m     df, feature_names, feature_types = \u001b[43m_transform_pandas_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    678\u001b[39m     handle = ctypes.c_void_p()\n\u001b[32m    679\u001b[39m     _check_call(\n\u001b[32m    680\u001b[39m         _LIB.XGDMatrixCreateFromColumnar(\n\u001b[32m    681\u001b[39m             df.array_interface(),\n\u001b[32m   (...)\u001b[39m\u001b[32m    686\u001b[39m         )\n\u001b[32m    687\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyterenv/lib/python3.12/site-packages/xgboost/data.py:640\u001b[39m, in \u001b[36m_transform_pandas_df\u001b[39m\u001b[34m(data, enable_categorical, feature_names, feature_types, meta)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data.columns) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _matrix_meta:\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataFrame for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot have multiple columns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m feature_names, feature_types = \u001b[43mpandas_feature_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    644\u001b[39m arrays = pandas_transform_data(data)\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m PandasTransformed(arrays), feature_names, feature_types\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyterenv/lib/python3.12/site-packages/xgboost/data.py:409\u001b[39m, in \u001b[36mpandas_feature_info\u001b[39m\u001b[34m(data, meta, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m    407\u001b[39m             new_feature_types.append(_pandas_dtype_mapper[dtype.name])\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m             \u001b[43m_invalid_dataframe_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    412\u001b[39m     feature_types = new_feature_types\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyterenv/lib/python3.12/site-packages/xgboost/data.py:372\u001b[39m, in \u001b[36m_invalid_dataframe_dtype\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    370\u001b[39m type_err = \u001b[33m\"\u001b[39m\u001b[33mDataFrame.dtypes for data must be int, float, bool or category.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_err\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_ENABLE_CAT_ERR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:MS Zoning: object, Street: object, Alley: object, Lot Shape: object, Land Contour: object, Utilities: object, Lot Config: object, Land Slope: object, Neighborhood: object, Condition 1: object, Condition 2: object, Bldg Type: object, House Style: object, Roof Style: object, Roof Matl: object, Exterior 1st: object, Exterior 2nd: object, Mas Vnr Type: object, Exter Qual: object, Exter Cond: object, Foundation: object, Bsmt Qual: object, Bsmt Cond: object, Bsmt Exposure: object, BsmtFin Type 1: object, BsmtFin Type 2: object, Heating: object, Heating QC: object, Central Air: object, Electrical: object, Kitchen Qual: object, Functional: object, Fireplace Qu: object, Garage Type: object, Garage Finish: object, Garage Qual: object, Garage Cond: object, Paved Drive: object, Pool QC: object, Fence: object, Misc Feature: object, Sale Type: object, Sale Condition: object"
     ]
    }
   ],
   "source": [
    "# Grid Search with XGBoost\n",
    "\n",
    "import pandas as pd               # Load pandas for data manipulation\n",
    "import xgboost as xgb             # Load XGBoost library\n",
    "import numpy as np                # Load NumPy for numerical operations\n",
    "from sklearn.model_selection import GridSearchCV  # Import GridSearchCV for hyperparameter tuning\n",
    "\n",
    "housing_data = pd.read_csv(\"AmesHousing.csv\")  \n",
    "# Load the preprocessed Ames housing dataset from CSV into a DataFrame\n",
    "\n",
    "X, y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]  \n",
    "# Split the dataset into features (X) and target (y). Assumes last column is the target variable.\n",
    "\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)  \n",
    "# Convert the data into XGBoost’s optimized DMatrix format (optional for scikit-learn interface, but useful for native XGBoost)\n",
    "\n",
    "gbm_param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 0.9],   # Try 4 different learning rates (eta)\n",
    "    'n_estimators': [200],                   # Fix number of boosting rounds to 200\n",
    "    'subsample': [0.3, 0.5, 0.9]             # Try 3 different subsample ratios\n",
    "}\n",
    "# Define the grid of hyperparameters to search over. Total combinations = 4 × 1 × 3 = 12\n",
    "\n",
    "gbm = xgb.XGBRegressor()  \n",
    "# Create an XGBoost regressor object using scikit-learn API\n",
    "\n",
    "grid_mse = GridSearchCV(\n",
    "    estimator=gbm,                   # The model to tune\n",
    "    param_grid=gbm_param_grid,       # The hyperparameter grid\n",
    "    scoring='neg_mean_squared_error',# Use negative MSE as scoring (scikit-learn convention)\n",
    "    cv=4,                            # Use 4-fold cross-validation\n",
    "    verbose=1                        # Print progress during training\n",
    ")\n",
    "# Set up the grid search with cross-validation and scoring metric\n",
    "grid_mse.fit(X, y)  \n",
    "# Fit the grid search object to the data. Trains 12 models (one for each parameter combo) using 4-fold CV.\n",
    "\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)  \n",
    "# Print the best combination of hyperparameters found during grid search\n",
    "\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))  \n",
    "# Convert the best negative MSE score to RMSE and print it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e52408-0be0-4ef6-82b5-bd5f5849a914",
   "metadata": {},
   "source": [
    "## Day 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6c91a-d18f-48b6-a201-aec8fdad92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load data\n",
    "housing_data = pd.read_csv(\"AmesHousing.csv\")\n",
    "X = housing_data[housing_data.columns.tolist()[:-1]]\n",
    "y = housing_data[housing_data.columns.tolist()[-1]]\n",
    "\n",
    "# Define parameter grid\n",
    "gbm_param_grid = {\n",
    "    'learning_rate': np.arange(0.05, 1.05, 0.05),\n",
    "    'n_estimators': [200],\n",
    "    'subsample': np.arange(0.05, 1.05, 0.05)\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "gbm = XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "randomized_mse = RandomizedSearchCV(\n",
    "    estimator=gbm,\n",
    "    param_distributions=gbm_param_grid,\n",
    "    n_iter=25,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=4,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "# Output results\n",
    "print(\"Best parameters found:\", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found:\", np.sqrt(np.abs(randomized_mse.best_score_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa3c823-07b6-463d-86b8-08dbb5cc4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_param_grid = {\n",
    "'colsample_bytree': [0.3, 0.7],\n",
    "'n_estimators': [50],\n",
    "'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator= gbm,\n",
    "param_grid=gbm_param_grid,\n",
    "scoring= 'neg_mean_squared_error',\n",
    "cv =4,\n",
    "verbose = 1)\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X,y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1c134-7e7f-46a4-8116-3403d6e2a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_param_grid = {\n",
    "'n_estimators': [25],\n",
    "'max_depth': range(2,12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(estimator =gbm,\n",
    "param_distributions = gbm_param_grid,\n",
    "scoring='neg_mean_squared_error',\n",
    "cv =4, \n",
    "n_iter = 5,\n",
    "verbose =1 )\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X,y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc897cd5-c5ff-4390-8a6b-c18b7b7f4fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn. ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b426ee-2696-4490-85c2-bf54c07a254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"crime\", \"zone\", \"industry\", \"charles\", \"no\", \"rooms\",\n",
    "\"age\", \"distance\", \"radial\", \"tax\", \"pupil\", \"aam\", \"lower\", \"med_price\"]\n",
    "data = pd.read_csv(\"boston_housing.csv\", names=names)\n",
    "X, y = data.iloc[:,:-1], data.iloc[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83a7b4-4b61-4a89-bfde-3078af72172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "X, y = data.iloc[:, :-1], data.iloc[:, -1]\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"st_scaler\", StandardScaler()),\n",
    "    (\"rf_model\", RandomForestRegressor())\n",
    "])\n",
    "\n",
    "scores = cross_val_score(rf_pipeline, X_encoded, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "final_avg_rmse = np.mean(np.sqrt(np.abs(scores)))\n",
    "print(\"Final Average RMSE:\", round(final_avg_rmse, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d168bd-7c55-49a8-b128-171d8d856ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1517e1d7-4ebb-42ad-99ab-b55f67512ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee90f374-2f19-40c3-910b-c906561f88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"AmesHousing.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=\"SalePrice\")\n",
    "y = df[\"SalePrice\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fbb083a-dabf-4c1f-bc3c-38cd96fcf5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "X = X.fillna(\"Missing\")  # for categorical\n",
    "X = X.fillna(X.mean(numeric_only=True))  # for numeric\n",
    "\n",
    "# Convert to dictionary format\n",
    "df_dict = X.to_dict(orient=\"records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32098243-ebab-4637-9793-a6280a65ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_encoded = dv.fit_transform(df_dict)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4098c0d3-f2f3-4738-8629-3d38e80aef47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test RMSE: 24966.9\n",
      "Final Test R²: 0.9223\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train model\n",
    "model = XGBRegressor(objective='reg:squarederror', n_estimators=200, learning_rate=0.1, subsample=0.8)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Final Test RMSE:\", round(rmse, 2))\n",
    "print(\"Final Test R²:\", round(r2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00f3c6e-fa0b-4ef9-8a44-99bb25530e8b",
   "metadata": {},
   "source": [
    "# Day 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b887ef9-2b41-48ee-b55d-1185c77a538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(\"chronic_kidney_disease.arff\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "kidney_data = pd.DataFrame(data)\n",
    "\n",
    "# Decode byte strings to regular strings\n",
    "for col in kidney_data.select_dtypes([object]):\n",
    "    kidney_data[col] = kidney_data[col].str.decode(\"utf-8\")\n",
    "\n",
    "# Target and features\n",
    "kidney_target_name = \"class\"\n",
    "kidney_feature_names = [col for col in kidney_data.columns if col != kidney_target_name]\n",
    "X = kidney_data[kidney_feature_names]\n",
    "y = kidney_data[kidney_target_name].apply(lambda x: 1 if x == \"ckd\" else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fa938a-ccda-4402-b329-ec0db066fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Identify categorical and numeric columns\n",
    "categorical_columns = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "non_categorical_columns = X.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "\n",
    "# Numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "    [([col], SimpleImputer(strategy=\"median\")) for col in non_categorical_columns],\n",
    "    input_df=True,\n",
    "    df_out=True\n",
    ")\n",
    "\n",
    "# Categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "    [(col, SimpleImputer(strategy=\"most_frequent\")) for col in categorical_columns],\n",
    "    input_df=True,\n",
    "    df_out=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422fd053-753a-49f3-a6a7-4ff9b5e3c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"featureunion\", numeric_categorical_union),\n",
    "    (\"dictifier\", Dictifier()),\n",
    "    (\"vectorizer\", DictVectorizer(sort=False)),\n",
    "    (\"clf\", xgb.XGBClassifier(max_depth=3))\n",
    "])\n",
    "\n",
    "# 3-fold cross-validation\n",
    "cross_val_scores = cross_val_score(\n",
    "    pipeline,\n",
    "    kidney_data[kidney_feature_names],\n",
    "    y,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28768e33-7efa-4ec0-9264-36291c7c8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.impute import SimpleImputer\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "    (\"num_mapper\", numeric_imputation_mapper),\n",
    "    (\"cat_mapper\", categorical_imputation_mapper)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26bf241-4a98-4dd0-a033-8925b59133e9",
   "metadata": {},
   "source": [
    "# above code will return error as the sklearn_pandas is obsolete and not available now\n",
    "# below code is full implementation of xgboost with hyperparam tuning and for xgb full code all in pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a344d320-3d40-4daf-8053-6ad1ddde652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "179d1918-2ec3-40ac-bcb6-6d9aa32a4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the Boston Housing dataset\n",
    "df = pd.read_csv(\"boston_housing.csv\")\n",
    "\n",
    "# Split into features and target\n",
    "X = df.drop(\"MEDV\", axis=1)\n",
    "y = df[\"MEDV\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31b92778-85fd-4d74-a452-ae4d553b9ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Pipeline and Tuning\n",
    "xgb_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"xgb_model\", xgb.XGBRegressor(objective=\"reg:squarederror\"))\n",
    "])\n",
    "\n",
    "xgb_param_grid = {\n",
    "    \"xgb_model__subsample\": [0.5, 0.7, 1.0],\n",
    "    \"xgb_model__max_depth\": [3, 5, 7],\n",
    "    \"xgb_model__colsample_bytree\": [0.5, 0.7, 1.0],\n",
    "    \"xgb_model__n_estimators\": [100, 200],\n",
    "    \"xgb_model__learning_rate\": [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_distributions=xgb_param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=5,\n",
    "    n_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea9762c6-ba0d-46c9-9399-1761214edccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XGBoost Best RMSE: 4.3590699644111135\n",
      "  XGBoost Best Estimator:\n",
      " Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('xgb_model',\n",
      "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "                              colsample_bylevel=None, colsample_bynode=None,\n",
      "                              colsample_bytree=1.0, device=None,\n",
      "                              early_stopping_rounds=None,\n",
      "                              enable_categorical=False, eval_metric=None,\n",
      "                              feature_types=None, feature_weights=None,\n",
      "                              gamma=None, grow_policy=None,\n",
      "                              importance_type=None,\n",
      "                              interaction_constraints=None, learning_rate=0.2,\n",
      "                              max_bin=None, max_cat_threshold=None,\n",
      "                              max_cat_to_onehot=None, max_delta_step=None,\n",
      "                              max_depth=3, max_leaves=None,\n",
      "                              min_child_weight=None, missing=nan,\n",
      "                              monotone_constraints=None, multi_strategy=None,\n",
      "                              n_estimators=100, n_jobs=None,\n",
      "                              num_parallel_tree=None, ...))])\n"
     ]
    }
   ],
   "source": [
    "xgb_search.fit(X, y)\n",
    "xgb_rmse = np.sqrt(-xgb_search.best_score_)\n",
    "print(\" XGBoost Best RMSE:\", xgb_rmse)\n",
    "print(\"  XGBoost Best Estimator:\\n\", xgb_search.best_estimator_)\n",
    "\n",
    "\n",
    "# GBM Pipeline and Tuning\n",
    "gbm_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"gbm_model\", GradientBoostingRegressor())\n",
    "])\n",
    "\n",
    "gbm_param_grid = {\n",
    "    \"gbm_model__subsample\": [0.5, 0.7, 1.0],\n",
    "    \"gbm_model__max_depth\": [3, 5, 7],\n",
    "    \"gbm_model__n_estimators\": [100, 200],\n",
    "    \"gbm_model__learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"gbm_model__min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "gbm_search = RandomizedSearchCV(\n",
    "    estimator=gbm_pipeline,\n",
    "    param_distributions=gbm_param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=5,\n",
    "    n_iter=10,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0819bc89-519b-4c23-9cc7-eeabd534dcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM Best RMSE: 4.070759769939876\n",
      "GBM Best Estimator:\n",
      " Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('gbm_model', GradientBoostingRegressor(subsample=0.7))])\n"
     ]
    }
   ],
   "source": [
    "gbm_search.fit(X, y)\n",
    "gbm_rmse = np.sqrt(-gbm_search.best_score_)\n",
    "print(\"GBM Best RMSE:\", gbm_rmse)\n",
    "print(\"GBM Best Estimator:\\n\", gbm_search.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb5d50d-a854-4911-94a7-7eac70fa95e1",
   "metadata": {},
   "source": [
    "# Termino de informe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d185dea-f2e1-4983-8b59-0aa52c9bc574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583baab-3b3f-4da4-903c-8a2c0785c3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b306afc0-84a6-439d-9a14-3d74e63f7d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
