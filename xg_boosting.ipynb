{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c1260ac-4d7a-4af8-ba0d-384e2bad6dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7743f3-554e-4a55-bafd-3d9677a7d8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30019/4072909886.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  churn_data['TotalCharges'].fillna(churn_data['TotalCharges'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load the dataset (make sure the CSV is in your working directory)\n",
    "churn_data = pd.read_csv('Telco-Customer-Churn.csv')\n",
    "\n",
    "# Step 2: Drop unnecessary column (customerID is not useful for prediction)\n",
    "churn_data = churn_data.drop('customerID', axis=1)\n",
    "\n",
    "# Step 3: Convert TotalCharges to numeric (it's stored as string; some values are blank)\n",
    "churn_data['TotalCharges'] = pd.to_numeric(churn_data['TotalCharges'], errors='coerce')\n",
    "# Fill NaN in TotalCharges (only 11 rows) with median\n",
    "churn_data['TotalCharges'].fillna(churn_data['TotalCharges'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d2f385-9f2d-4890-8dd6-f98d08df8723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rustamshrestha/jupyterenv/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [18:42:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.784244\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in churn_data.columns:\n",
    "    if churn_data[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        churn_data[column] = le.fit_transform(churn_data[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "# Now all columns are numeric\n",
    "X = churn_data.iloc[:, :-1]  # All features except last column\n",
    "y = churn_data.iloc[:, -1]   # Last column is 'Churn' (0 = No, 1 = Yes)\n",
    "\n",
    "# Step 5: Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=123\n",
    ")\n",
    "\n",
    "# Step 6: Train XGBoost classifier\n",
    "xg_cl = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    n_estimators=100,  # Increased for better performance\n",
    "    seed=123,\n",
    "    use_label_encoder=False,  # Suppress warning\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predict and evaluate\n",
    "preds = xg_cl.predict(X_test)\n",
    "accuracy = float(np.sum(preds == y_test)) / y_test.shape[0]\n",
    "print(\"Accuracy: %f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7487f10-46f3-44ab-9e02-3a0cee55db33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the DecisionTreeClassifier with max_depth=4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4, random_state=123)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = float(np.sum(y_pred_4 == y_test)) / y_test.shape[0]\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6c0971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-error-mean  train-error-std  test-error-mean  test-error-std\n",
      "0          0.055358         0.014118         0.098459        0.016569\n",
      "1          0.026367         0.003758         0.061524        0.013876\n",
      "2          0.012302         0.001236         0.061533        0.013930\n",
      "3          0.011427         0.002497         0.052752        0.015618\n",
      "4          0.008790         0.002494         0.052752        0.015618\n",
      "Accuracy: 0.9472\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create the DMatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\": \"binary:logistic\", \"max_depth\": 3}\n",
    "\n",
    "# Perform 3-fold cross-validation\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, \n",
    "                    metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {((1 - cv_results['test-error-mean']).iloc[-1]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba514a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
      "0        0.989659       0.005199       0.959695      0.025255\n",
      "1        0.995100       0.003751       0.972271      0.023940\n",
      "2        0.997122       0.002032       0.973122      0.025047\n",
      "3        0.997103       0.002030       0.982087      0.013069\n",
      "4        0.997832       0.001851       0.982567      0.013554\n",
      "AUC: 0.9826\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create the DMatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\": \"binary:logistic\", \"max_depth\": 3}\n",
    "\n",
    "# Perform 3-fold cross-validation with AUC metric\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, \n",
    "                    metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print(f\"AUC: {(cv_results['test-auc-mean']).iloc[-1]:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fd3d9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 45.1356\n",
      "MAE: 35.6638\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Generate synthetic regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=4, noise=0.1, random_state=123)\n",
    "\n",
    "# Split the data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define parameters\n",
    "params = {\"objective\": \"reg:squarederror\", \"max_depth\": 3, \"eta\": 0.1}\n",
    "\n",
    "# Train the model\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=10)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = xgb_model.predict(dtest)\n",
    "\n",
    "# Compute RMSE and MAE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4910dd1-a561-4d43-ba63-1507cb44416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import xgboost as xgb                 # XGBoost library for gradient boosting\n",
    "import pandas as pd                   # For data manipulation\n",
    "import numpy as np                    # For numerical operations\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  # For evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "021100f3-75db-4fce-9aea-63370474cb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Metrics:\n",
      "Mean Absolute Error (MAE): 2.85\n",
      "Mean Squared Error (MSE): 21.20\n",
      "R² Score: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = boston_data.iloc[:, :-1]  # All columns except the last\n",
    "y = boston_data.iloc[:, -1]   # Last column as target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=10, seed=123)\n",
    "\n",
    "# Train the model\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mae = mean_absolute_error(y_test, preds)\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)\n",
    "\n",
    "# Print accuracy metrics\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3266c47-fb00-408d-97a6-4cb2b5a6a2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Metrics:\n",
      "Mean Absolute Error (MAE): 4.19\n",
      "Mean Squared Error (MSE): 37.06\n",
      "R² Score: 0.55\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = boston_data.iloc[:, :-1]  # All columns except the last\n",
    "y = boston_data.iloc[:, -1]   # Last column as target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "\n",
    "DM_test = xgb.DMatrix(data=X_test, label=y_test)  # Use y_test here instead of y_train\n",
    "\n",
    "params= {\"booster\":\"gblinear\", \"objective\":\"reg:squarederror\"}\n",
    "xg_reg = xgb.train(params=params, dtrain=DM_train, num_boost_round=10)\n",
    "\n",
    "preds =xg_reg.predict(DM_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mae = mean_absolute_error(y_test, preds)\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)\n",
    "\n",
    "# Print accuracy metrics\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83420eaa-e1bc-41c6-a38d-0e05404e3067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.604776\n"
     ]
    }
   ],
   "source": [
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "287bd72c-b673-44b0-b8f9-910303eff2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 6.454039\n"
     ]
    }
   ],
   "source": [
    "# Convert the training and testing sets into DMatrixes\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "DM_test = xgb.DMatrix(data=X_test, label=y_test) \n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"booster\": \"gblinear\", \"objective\": \"reg:squarederror\"} \n",
    "\n",
    "# Train the model\n",
    "xg_reg = xgb.train(params=params, dtrain=DM_train, num_boost_round=5)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xg_reg.predict(DM_test) #\n",
    "\n",
    "# Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "973620e2-9e6c-42bc-ae9f-af04fa746f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0         6.989180        0.032758        7.188751       0.188563\n",
      "1         5.459640        0.032484        5.959252       0.216005\n",
      "2         4.369413        0.028176        5.150507       0.258300\n",
      "3         3.612622        0.043512        4.570613       0.344081\n",
      "4         3.071021        0.044801        4.296591       0.447778\n",
      "4    4.296591\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30417be6-21a9-40d6-9106-716a2a6f52c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE as a function of alpha:\n",
      "   alpha      rmse\n",
      "0      1  3.685441\n",
      "1     10  3.761246\n",
      "2    100  4.461392\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = boston_data.iloc[:, :-1]\n",
    "y = boston_data.iloc[:, -1]\n",
    "\n",
    "# Create DMatrix\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Set base parameters\n",
    "params = {\"objective\": \"reg:squarederror\", \"max_depth\": 4}\n",
    "\n",
    "# L1 regularization values to test\n",
    "l1_params = [1, 10, 100]\n",
    "rmses_l1 = []\n",
    "\n",
    "# Loop through each alpha value\n",
    "for reg in l1_params:\n",
    "    params[\"alpha\"] = reg  # Set L1 regularization\n",
    "    cv_results = xgb.cv(\n",
    "        dtrain=boston_dmatrix,\n",
    "        params=params,\n",
    "        nfold=4,\n",
    "        num_boost_round=10,\n",
    "        metrics=\"rmse\",\n",
    "        as_pandas=True,\n",
    "        seed=123\n",
    "    )\n",
    "    # Extract final RMSE\n",
    "    final_rmse = cv_results[\"test-rmse-mean\"].tail(1).values[0]\n",
    "    rmses_l1.append(final_rmse)\n",
    "\n",
    "# Display results\n",
    "print(\"Best RMSE as a function of alpha:\")\n",
    "print(pd.DataFrame(list(zip(l1_params, rmses_l1)), columns=[\"alpha\", \"rmse\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242327e4-7d41-4c46-a3fb-20aea3b00e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":2}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot all trees in a loop\n",
    "for i in range(10):\n",
    "    xgb.plot_tree(xg_reg, num_trees=i)\n",
    "    plt.title(f\"Tree {i}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cadd10-984a-40cd-bc01-cdee82e69ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the feature importances\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668cb07c-54d0-4ad8-bb24-e5c02cf7858a",
   "metadata": {},
   "source": [
    "# Day 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e12bcd-5633-4e2e-af88-7c32d5d0d988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454217f9-bbaf-45a2-ac78-f3ed232b7dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc79553f-b6c0-42b2-804d-2b3add7fc9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146edfec-5c7a-462d-a814-3b7587058e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95bf69-298f-4268-9241-c11a66d706e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e42eed-ef6f-4343-8c70-5c14446eb4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
